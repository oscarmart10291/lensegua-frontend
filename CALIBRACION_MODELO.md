# üéØ Gu√≠a de Calibraci√≥n del Modelo de Lengua de Se√±as

## ‚ö†Ô∏è Problemas Corregidos

Se identificaron y corrigieron **3 errores cr√≠ticos** que causaban la p√©rdida de precisi√≥n:

### 1. ‚ùå Doble aplicaci√≥n de Softmax
**Problema:** El c√≥digo aplicaba `tf.softmax()` sobre la salida del modelo, pero el modelo **ya tiene softmax** en su √∫ltima capa.

**Efecto:** Distorsi√≥n completa de las probabilidades, haciendo que el modelo sea impreciso.

**Soluci√≥n:** ‚úÖ Eliminada la doble aplicaci√≥n de softmax en las l√≠neas 248, 261 y 277 de `PracticeModal.tsx`

### 2. ‚ùå Mapeado Incompleto de Clases
**Problema:**
- El modelo tiene **28 clases de salida**
- El `class_index.json` original solo ten√≠a **21 letras**
- Faltaban: D, F, J, √ë, P, S, CH

**Soluci√≥n:** ‚úÖ Se actualiz√≥ `class_index.json` con las 28 clases completas

### 3. ‚ö†Ô∏è Verificaci√≥n del Preprocesamiento
**Estado:** El preprocesamiento usa correctamente 42 valores (21 landmarks √ó 2 coordenadas X,Y)

---

## üîß C√≥mo Calibrar el Modelo

**IMPORTANTE:** El mapeo actual asume que las letras faltantes (D, F, J, √ë, P, S, CH) est√°n en los √≠ndices 21-27. **Debes verificar esto con tu modelo original.**

### Paso 1: Identificar el Orden de Entrenamiento

Necesitas saber en qu√© orden entrenaste las clases. Por ejemplo:
```python
# En tu c√≥digo de entrenamiento de Keras/TensorFlow
# Busca algo como:
class_names = ['A', 'B', 'C', 'D', 'E', 'F', ...]  # ¬øEn qu√© orden?
# o
label_encoder.classes_  # Si usaste LabelEncoder
```

### Paso 2: Usar el Modo Calibraci√≥n en la App

1. Abre la aplicaci√≥n y ve a cualquier m√≥dulo de pr√°ctica
2. Haz clic en el bot√≥n **"Calibrar (C)"** en el panel de debug
3. Para cada letra:
   - Haz la se√±a est√°tica
   - Observa el **√≠ndice top** que muestra el modelo
   - Si el √≠ndice no coincide, haz clic en **"Asignar top ‚Üí [letra]"**
4. Cuando termines, haz clic en **"Descargar JSON"**
5. Reemplaza el archivo `public/models/estatico_last/class_index.json`

### Paso 3: Verificar el Preprocesamiento

Aseg√∫rate de que el preprocesamiento en producci√≥n coincida con el de entrenamiento:

#### En tu c√≥digo de entrenamiento (Python):
```python
def preprocess_landmarks(landmarks):
    # 1. Normalizar respecto a la mu√±eca (punto 0)
    wrist = landmarks[0]
    landmarks = landmarks - wrist

    # 2. Escalar por bounding box
    bbox_width = landmarks[:, 0].max() - landmarks[:, 0].min()
    bbox_height = landmarks[:, 1].max() - landmarks[:, 1].min()
    scale = np.hypot(bbox_width, bbox_height)
    landmarks = landmarks / scale

    # 3. Aplanar a vector (solo X,Y, sin Z)
    return landmarks[:, :2].flatten()  # Shape: (42,)
```

#### En la app (JavaScript - ya implementado):
- ‚úÖ Normalizaci√≥n respecto a mu√±eca (l√≠nea 309)
- ‚úÖ Escalado por bounding box (l√≠neas 312-315)
- ‚úÖ Solo usa X,Y (42 valores) (l√≠nea 325)

---

## üìä Mapeo de Clases Est√°ticas (21 clases)

El mapeo correcto para se√±as **est√°ticas** es:

```json
{
  "A": 0, "B": 1, "C": 2, "E": 3, "G": 4, "H": 5, "I": 6,
  "K": 7, "L": 8, "M": 9, "N": 10, "O": 11, "Q": 12, "R": 13,
  "T": 14, "U": 15, "V": 16, "W": 17, "X": 18, "Y": 19, "Z": 20
}
```

**‚úÖ Este es el orden CORRECTO de entrenamiento.**

**‚ö†Ô∏è PROBLEMA DETECTADO:** El modelo tiene 28 unidades de salida pero solo 21 clases fueron entrenadas. Los √≠ndices 21-27 son "clases fantasma" y ser√°n ignorados autom√°ticamente por el c√≥digo.

---

## üêç Script de Python para Verificar el Modelo

Si tienes el modelo original en Keras, ejecuta esto:

```python
import tensorflow as tf
import numpy as np

# Cargar modelo original
model = tf.keras.models.load_model('ruta/al/modelo.h5')

# Ver estructura
print("Input shape:", model.input_shape)   # Debe ser (None, 42)
print("Output shape:", model.output_shape) # Debe ser (None, 28)

# Verificar √∫ltima activaci√≥n
last_layer = model.layers[-1]
print("√öltima activaci√≥n:", last_layer.activation)  # Debe ser 'softmax'

# Ver clases (si guardaste los labels)
# Busca un archivo como 'class_names.npy' o similar
class_names = np.load('class_names.npy')  # O como lo hayas guardado
print("Clases:", class_names)

# Crear mapeo correcto
class_index = {name: idx for idx, name in enumerate(class_names)}
print("Mapeo correcto:", class_index)
```

---

## üß™ Probar las Correcciones

1. **Prueba con se√±as conocidas:**
   - Haz la se√±a de "A" ‚Üí Debe dar alta confianza para √≠ndice 0
   - Haz la se√±a de "B" ‚Üí Debe dar alta confianza para √≠ndice 1
   - etc.

2. **Compara con Keras:**
   ```python
   # En Python
   import numpy as np

   # Simula los landmarks que est√°s enviando desde la app
   landmarks_test = np.array([...])  # 21 puntos √ó 2 = shape (42,)

   prediction = model.predict(landmarks_test.reshape(1, 42))
   predicted_class = np.argmax(prediction)
   confidence = prediction[0][predicted_class]

   print(f"Clase predicha: {predicted_class} ({class_names[predicted_class]})")
   print(f"Confianza: {confidence * 100:.2f}%")
   ```

3. **Usa el modo debug de la app:**
   - Observa si "top" coincide con la letra esperada
   - Verifica que la confianza sea > 70% con buena iluminaci√≥n

---

## üìù Notas Importantes

### Coordenadas Z
- El modelo usa **solo X,Y** (42 valores)
- MediaPipe proporciona X,Y,Z (63 valores)
- La configuraci√≥n `USE_Z_BY_F` est√° correctamente configurada para ignorar Z

### Mirror X
- `MIRROR_X: true` invierte la coordenada X
- Esto es correcto si entrenaste con c√°mara frontal (selfie mode)
- Si entrenaste con im√°genes sin espejo, cambia a `false` en l√≠nea 19

### Rotaci√≥n
- `ROT_ALIGN: false` est√° desactivado
- Si entrenaste con alineaci√≥n rotacional, act√≠valo en l√≠nea 22

---

## üÜò Troubleshooting

### El modelo sigue siendo impreciso:

1. **Verifica el orden de las clases** con tu c√≥digo de entrenamiento
2. **Revisa el preprocesamiento:** ¬øNormalizaste de la misma manera?
3. **Comprueba MIRROR_X:** ¬øEntrenaste con im√°genes espejadas?
4. **Iluminaci√≥n:** Prueba con buena luz directa
5. **Distancia de la c√°mara:** Mant√©n la mano a ~50cm de la c√°mara

### C√≥mo comparar preprocesamiento:

Exporta landmarks desde la app (agrega esto temporalmente en l√≠nea 299):
```typescript
console.log('Landmarks procesados:', frameVec);
```

Luego compara con Python:
```python
# Tus landmarks de Python
python_landmarks = preprocess_landmarks(hand_data)
print('Landmarks Python:', python_landmarks)
```

Deben ser muy similares (diferencias < 0.01).

---

## ‚úÖ Checklist de Validaci√≥n

- [ ] Verifiqu√© el orden de clases en mi c√≥digo de entrenamiento
- [ ] Actualic√© `class_index.json` con el mapeo correcto
- [ ] Prob√© al menos 5 se√±as diferentes
- [ ] La confianza es > 70% con buena iluminaci√≥n
- [ ] El √≠ndice "top" coincide con la letra esperada
- [ ] Compar√© el preprocesamiento entre Python y JavaScript
- [ ] Verifiqu√© que `MIRROR_X` es correcto para mi caso

---

**¬øNecesitas m√°s ayuda?** Revisa los logs del navegador (F12 ‚Üí Console) para ver errores de TensorFlow.js.

---

## üö® Problema: 28 Unidades de Salida vs 21 Clases Entrenadas

### El Problema

Tu modelo actual tiene una **inconsistencia cr√≠tica**:

- **Modelo:** 28 unidades en la capa de salida (Dense)
- **Dataset:** Solo 21 clases est√°ticas entrenadas (A-Z sin D, F, J, √ë, P, S)

Esto significa que hay **7 neuronas "fantasma"** (√≠ndices 21-27) que:
- ‚ùå Nunca fueron entrenadas con datos reales
- ‚ùå Generan predicciones aleatorias/basura
- ‚ùå Pueden interferir con las predicciones correctas

### Soluci√≥n Temporal (Ya Implementada)

El c√≥digo ha sido **modificado autom√°ticamente** para:
- ‚úÖ Ignorar √≠ndices 21-27 al calcular el top-1
- ‚úÖ Solo considerar √≠ndices 0-20 (clases v√°lidas)
- ‚úÖ Filtrar predicciones basura

**Ubicaci√≥n:** `src/components/PracticeModal.tsx` l√≠nea 283-292

### Soluci√≥n Permanente (Recomendada)

**Re-entrenar el modelo con la arquitectura correcta:**

```python
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Dropout

# ‚ö†Ô∏è IMPORTANTE: 21 clases, NO 28
NUM_CLASES = 21

class_names = [
    'A', 'B', 'C', 'E', 'G', 'H', 'I', 'K', 'L', 'M', 'N',
    'O', 'Q', 'R', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z'
]

# Arquitectura corregida (basada en tu model.json)
model = Sequential([
    Dense(256, activation='relu', input_shape=(42,)),
    Dropout(0.3),
    Dense(128, activation='relu'),
    Dropout(0.2),
    Dense(64, activation='relu'),
    Dense(NUM_CLASES, activation='softmax')  # ‚úÖ 21 clases
])

model.compile(
    optimizer=tf.keras.optimizers.Adam(learning_rate=0.000125),
    loss='sparse_categorical_crossentropy',
    metrics=['accuracy']
)

print(f"‚úÖ Modelo corregido con {NUM_CLASES} clases de salida")

# Entrenar con tu dataset...
# model.fit(X_train, y_train, epochs=100, validation_data=(X_val, y_val))

# Exportar a TensorFlow.js
import tensorflowjs as tfjs
tfjs.converters.save_keras_model(model, 'models/estatico_corregido')
```

### Diagn√≥stico del Problema

Usa el script de diagn√≥stico para investigar:

```bash
# Analizar el modelo Keras original
python scripts/diagnosticar_modelo.py --model modelo_original.h5

# Analizar el dataset
python scripts/diagnosticar_modelo.py --dataset dataset/estaticas/

# Analizar el modelo TF.js exportado
python scripts/diagnosticar_modelo.py --tfjs public/models/estatico_last/

# An√°lisis completo
python scripts/diagnosticar_modelo.py \
  --model modelo_original.h5 \
  --dataset dataset/estaticas/ \
  --tfjs public/models/estatico_last/
```

El script te dir√°:
- ‚úÖ Cu√°ntas clases hay en tu dataset
- ‚úÖ Cu√°ntas unidades tiene tu modelo
- ‚úÖ Si hay discrepancia entre modelo y dataset
- ‚úÖ C√≥digo para corregir el problema

### Por Qu√© Ocurri√≥ Esto

Posibles causas:

1. **Error al definir la √∫ltima capa:**
   ```python
   # ‚ùå Pusiste 28 en lugar de 21
   model.add(Dense(28, activation='softmax'))
   ```

2. **Contaste mal las clases:**
   - Pensaste que eran 28 (26 letras + CH + LL)
   - Pero realmente solo entrenaste 21

3. **Usaste un modelo pre-definido:**
   - Copiaste c√≥digo de otro proyecto que usaba 28 clases
   - Olvidaste ajustar la √∫ltima capa

### ¬øPuedo Seguir Usando el Modelo Actual?

**S√≠, temporalmente:**
- ‚úÖ El c√≥digo ahora ignora los √≠ndices 21-27
- ‚úÖ Solo usa las 21 clases v√°lidas (0-20)
- ‚úÖ Deber√≠a funcionar con mejor precisi√≥n

**Pero es mejor re-entrenar porque:**
- ‚ö° Modelo m√°s peque√±o = m√°s r√°pido
- üìâ Menos probabilidad de errores
- üéØ Arquitectura limpia y correcta

### Checklist de Correcci√≥n

- [ ] Revis√© mi c√≥digo de entrenamiento
- [ ] Confirm√© que solo tengo 21 clases
- [ ] Ejecut√© `diagnosticar_modelo.py` para verificar
- [ ] Re-entren√© el modelo con Dense(21) en la salida
- [ ] Export√© correctamente a TensorFlow.js
- [ ] Reemplac√© el modelo en `public/models/estatico_last/`
- [ ] Verifiqu√© que la precisi√≥n mejor√≥

---
